\documentclass[sigconf,review,anonymous]{acmart}

%% \acmConference[ESEC/FSE 2020]{The 28th ACM Joint European
%% Software Engineering Conference and Symposium on the Foundations
%% of Software Engineering}{8 - 13 November, 2020}{Sacramento,
%% California, United States}

\AtBeginDocument{ \providecommand\BibTeX{{ \normalfont
B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\hyphenation{on-tol-o-gy ad-di-tion-al in-fra-struc-ture
an-a-lyze a-nal-y-sis}

\input{packages}
\input{macros}

\begin{document}

\title{Optimized ReRun to Find Time-Constrained Flaky Tests}

\author{First Author} \affiliation{\institution{First
Institution}} \email{first@email.com}

\author{Second Author} \affiliation{\institution{Second
Institution}} \email{second@email.com}

\author{Third Author} \affiliation{\institution{Third
Institution}} \email{third@email.com}

\begin{abstract}
% context
A test is said to be flaky when it non-deterministically passes or fails. Test flakiness negatively affects the effectiveness of regression testing and, consequently, impacts software evolution. 
% problem
Detecting test flakiness is an important and challenging problem. \rerun{} is the most popular approach in industry to mitigate test flakiness. It re-executes a test suite on a fixed code version multiple times, looking for inconsistent outputs across executions. Unfortunately, \rerun{} is costly and unreliable.
%needs several executions to capture low probability flaky tests.
% solution
This paper proposes \tname{}, a lightweight technique to improve the ability of \rerun{} to detect flaky tests. \tname{} adds noise in the execution environment, \eg{}, it adds stressor tasks to compete for the CPU or memory. It builds on the observations that concurrency is an important source of flakiness and that adding noise in the environment can interfere in event ordering and, consequently, influence test outputs.
% evaluation
We conducted experiments on a data set with \numflakytestsds{} flaky tests of \numprojects{} open source Android apps. Results are encouraging. We found that \tname{} detected $\sim$86\% of all flaky tests from our dataset in $\sim$10\% of the time \rerun{} needed for that task. Furthermore, we ran \tname{} on this same set of apps and found \numNewFlakies{} new flaky tests that \numReRuns{} re-executions of the test suite missed. 
\end{abstract}

\keywords{test flakiness, regression testing, noise}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10011007.10011074.10011099.10011102.10011103</concept_id>
       <concept_desc>Software and its engineering~Software testing and debugging</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Software testing and debugging}

\maketitle

\section{Introduction}
\label{sec:intro}
A test is said to be \emph{flaky} when it non-deterministically passes or fails depending on the running environment~\cite{john-mico-google2016}. For example, a test may fail or pass depending on the availability of a server that the test itself has spawned---the test passes if the server is up-and-running at the point the test makes a request to the server and it fails, otherwise. 

Test flakiness hurts software testing practice in multiple ways. For example, Thorve~\etal{}~\cite{thorve2018empirical} found that, because of test flakiness, developers lost trust in test results, choosing to ignore test failures altogether sometimes. Even ignoring failures of known flaky tests or ignoring previously classified flaky tests (\ie{}, not executing those tests) can be dangerous as those tests could reveal bugs in code. Furthemore, ignoring flakiness can produce the effect of observing even more failures to be analyzed during software evolution. For example, Rahman and Rigby~\cite{rahman-rigby-fse2018} found that when developers ignored flaky test failures during a build, the deployed build experienced more crashes than builds that did not contain any flaky test failures. 
Test flakiness is a huge problem in industry~\cite{,DBLP:conf/scam/HarmanO18}. Most test failures at Google are due to flaky tests~\cite{john-mico-google2016,jeff-listfield-google2017}. At Microsoft, the presence of flaky tests also imposes a significant burden on developers. Wing~\etal{}~\cite{wing-etal-issta11} reported that 58 Microsoft developers involved in a survey considered flaky tests to be the second most important reason, out of 10 reasons, for slowing down software deployments. Finally, Facebook research considers detection of flakiness a priority~\cite{DBLP:conf/scam/HarmanO18}. 

\rerun\ is the most popular approach in industry to detect test flakiness~\cite{john-mico-google2016,flakiness-at-spotify}. It
reruns tests that failed for multiple times. A test that failed and then passed is considered flaky. The status of a test that persistently failed is unknown, but developers typically treat this scenario as a problem in application code as opposed to a bug in test code. Unfortunately, \rerun{} is unreliable and expensive. It is unreliable because it is hard to determine the number of reruns  necessary to find discrepancy in outputs. Is is expensive because rerunning tests consumes a lot of computing power. Google, for example, uses 2-16\% of its testing budget just to rerun flaky test~\cite{john-mico-google2016}. Researchers have proposed various techniques to identify flaky tests. For example, Bell~\etal{}~\cite{bell2018d} proposed Deflaker, a technique that determines that a test is flaky if the output of a test has changed even though there was no change in the code reachable from the execution trace. Note that Deflaker cannot capture flakiness in test cases not impacted by changes and that change-impacted flaky tests do not reveal flakiness necessarily. Pinto~\etal~\cite{pinto-etal-msr2020} proposed the use of text processing and classification to statically identify likely flaky test cases from the keywords they use. Important to note that the technique provides indication of likely flakiness, not a verdict. Overall, existing approaches suffer from different limitations. 

This paper proposes \tname, a lightweight technique to improve the ability of \rerun\ to detect flaky tests. \tname\ adds noise in the execution environment with the goal of provoking failures in time-constrained tests. For example, \tname{} adds stressor tasks to compete with the test execution for the CPU or memory. \tname{} builds on the observations that concurrency is an important source of flakiness and that adding noise in the environment can interfere in the ordering of events related to test execution and, consequently, influence on the test outputs. The process of detecting flaky tests consists of two steps. In the first \emph{offline} step, \tname{} uses a sample of tests known to be flaky to search for configurations of a noise generator to be used for revealing flakiness in new tests. To that end, \tname\ first builds a probability matrix encoding the relation between flaky tests and noise configurations. The matrix shows the probability of a test failing when executed with a given noise configuration. Then, \tname{} uses that matrix to search for sets of configurations that reveals the highest number of flaky tests. In the second \emph{online} step, \tname{} uses those configurations to find time-constrained flaky tests in the project of interest.

\Mar{improve this par. closer to submission...}
We conducted experiments on a data set with \numflakytestsds{} flaky tests of \numprojects{} open source Android apps. Preliminary results provide early evidence that \tname\ is promising. We found that \tname\ detected ∼86\% of all flaky tests from our data set in ∼10\% of the time \rerun\ needed for that task. Furthermore, we ran \tname\ on this same set of apps and found 46 new flaky tests that \rerun\ missed.

This paper makes the following contributions:

\begin{itemize}[topsep=.2ex,itemsep=.2ex,leftmargin=0.8em]

\item[\Contrib{}]\textbf{Approach.} This paper proposes a simple approach to find time-constrained flaky tests by introducing noise in test executions. \tname{} optimizes \rerun{}, the most popular approach in industry for finding flaky tests~\Fix{cite}.
  
\item[\Contrib{}]\textbf{Implementation.}The tool implementing \tname{} is publicly available online. 

%\Mar{Denini, if we have time it would be good to write some gradle/maven plugin. If not, we only make the optimization scripts available.}
  
\item[\Contrib{}]\textbf{Evaluation.} We evaluated \tname{} on Android apps. Results indicate that \Fix{...}

\item[\Contrib{}]\textbf{Artifacts.} Our tool, scripts, and
 data are publicly available at the following link \textbf{\artifactUrl{}}

\end{itemize}

\section{Examples}
\label{sec:example}

% \Mar{need to describe few compelling examples. @Denini, can you find two or three tests that we frequently detect flakiness (prob. higher than >.5) and it is difficult for rerun to find (prob. lower than .1)? @Leopoldo, can you dig into these examples and explain 1) why they are flaky (time constraints)?} \Leo{Working on Denini's examples.}

This section presents two examples of flaky tests in Android applications to motivate and illustrate \tname{}.

\subsection{AntennaPod}


\begin{figure*}[t]
\begin{lstlisting}[language=Java, caption=AntennaPod Test\Mar{Also, suggest to use highlighting to highlight important elements in the discussion.}, label=AntennaPodTest,escapechar=|]
@Test
public void testReplayEpisodeContinuousPlaybackOff() throws Exception {
 setContinuousPlaybackPreference(false);|\label{line:continuousplayback}|
 uiTestUtils.addLocalFeedData(true);|\label{line:adddata}|
 activityTestRule.launchActivity(new Intent());
 //Navigate to the first episode in the list of episodes and click
 openNavDrawer();|\label{line:nav-start}|
 onDrawerItem(withText(R.string.episodes_label)).perform(click());
 onView(isRoot()).perform(waitForView(withText(R.string.all_episodes_short_label), 1000));
 onView(withText(R.string.all_episodes_short_label)).perform(click());
 final List<FeedItem> episodes = DBReader.getRecentlyPublishedEpisodes(0, 10);
 Matcher<View> allEpisodesMatcher = allOf(withId(android.R.id.list), isDisplayed(), hasMinimumChildCount(2));
 onView(isRoot()).perform(waitForView(allEpisodesMatcher, 1000));
 onView(allEpisodesMatcher).perform(actionOnItemAtPosition(0, clickChildViewWithId(R.id.secondaryActionButton)));|\label{line:nav-finish}|
 FeedMedia media = episodes.get(0).getMedia();
 Awaitility.await().atMost(1, TimeUnit.SECONDS).until( () -> media.getId() == PlaybackPreferences.getCurrentlyPlayingFeedMediaId());|\label{line:awitibility}|
 //...
}
\end{lstlisting}
\end{figure*}

%, besides other technologies typically used in Android applications such as XML for user interface and Gradle for build automation

\sloppy
AntennaPod\footnote{\url{https://antennapod.org}} is an open source podcast manager for Android supporting episode download and streaming. 
The Android app is implemented in Java in $\sim$50KLOC.
As of the date of submission,\footnote{Revision SHA dd5234c as per the time of submission} the app had \numTestsAntenna{} GUI tests written in Espresso~\cite{espresso-2020} and UIAutomator~\cite{uiautomator-2020}.

\sloppy
Listing~\ref{AntennaPodTest} shows a simplified version of the test  \texttt{testReplayEpisodeContinuousPlaybackOff}, which checks whether a podcast episode can be played twice. The test uses the Awaitility library~\cite{awaitibility} to handle asynchronous events typically related to I/O (\eg{}, media playback). Execution of the statement at line~\ref{line:continuousplayback} turns the continuous playback option off, which has the effect of not playing the next episode (in the episode queue) automatically after playing an episode. Execution of the statement at line~\ref{line:adddata} then adds local data (\eg{}, podcast feeds, images, and episodes) to the app. The execution of the statements at lines~\ref{line:nav-start}--\ref{line:nav-finish} navigate through the GUI objects with the effect of playing the first episode in the queue. Line~\ref{line:awitibility} shows an example assertion of the Awaitibility library. The assertion checks if the episode is actually being played and indicates that test execution should wait for no longer than one second to verify this.

%\Mar{Leopoldo, vc. pode explicar o que aconteceu quando o (ultimo?) botao foi clicado? Algum servico *asincrono* custoso foi iniciado e a execucao continuou? Vc. pode elaborar um pouco mais? Por exemplo, indicando que servico foi este (playback?) e porque eh custoso (dado que arquivos de media estao na maquina?)?}

This test is flaky---it fails depending on the state of the environment. The test checks whether or not the episode is actually being played (line~\ref{line:awitibility}). When the play button is pressed (line~\ref{line:nav-finish}), the app runs a custom service in the background---\CodeIn{PlaybackService.java}\footnote{Look for "Thread" in the \CodeIn{PlaybackService.java} file of the AntennaPod app~\cite{antennapod-playback}.}---to load the media file from the file system and, subsequently, play the media to the user. These are typically expensive I/O operations. If the 1 second limit is reached, the execution of the assertion raises the exception (\texttt{ConditionTimeoutException}). We indeed observed that this test raises this exception in a heavy loaded (CPU/memory) environment.

\rerun{} does not perform well to identify flakiness in this test case. We ran this test for 50 times and found failures in 11 runs, \ie{}, 22\% of the cases. This example shows that, albeit practical and widely adopted in industry~\cite{Luo:2014:EAF:2635868.2635920,DBLP:conf/scam/HarmanO18,john-mico-google2016}, 
\rerun{} is either (1) ineffective at finding flakiness (if the maximum number of re-executions is set too low) or (2) costly (if failures manifest with low-probability). In this case, one would need to execute the test five times to detect the failure with high probability. 

%The extra cost affects every test that fails. The main problem is to decide the number of re-executions to decide whether a test is flaky or not.

When configured to execute the test suite using only the most effective noisy configuration, \tname{} detects this test case in one execution of the test suite. Furthermore, we re-executed the test case for 50 times with this configuration and were able to detect flakiness in all of them. Although each individual execution of the test suite is more expensive with \tname, it requires fewer re-executions of the test suite to detect flakiness. For comparison, one non-noisy executions of this test casa takes 11.08s whereas one execution of the test case under \tname{}'s noisy environment takes 20.2s, \ie{}, the execution without noise is $\sim$2x faster in this case. Comparing the total execution time, however, \tname{} enabled the discovery of flakiness 2.74x (=55.4/20.2) faster and, more importantly, it required a single execution for that.

%(when the method \CodeIn{getCurrentlyPlayingFeedMediaId()} returns the same value as the method \CodeIn{media.getId()}) 



% We also use it to establish that the test waits at most 5 seconds to check whether the episode has finished playing (dummy test data consists of very small audio files).



%  Awaitility.await().atMost(1, TimeUnit.SECONDS).until(
%         () -> media.getId() == PlaybackPreferences.getCurrentlyPlayingFeedMediaId());
%  Awaitility.await().atMost(5, TimeUnit.SECONDS).until(
%         () -> media.getId() != PlaybackPreferences.getCurrentlyPlayingFeedMediaId());
%  startLocalPlayback();
%  Awaitility.await().atMost(1, TimeUnit.SECONDS).until(
%         () -> media.getId() == PlaybackPreferences.getCurrentlyPlayingFeedMediaId());   



\begin{figure}[t!]
    \centering
    \includegraphics[width=5cm]{figs/antennapod.png}
    \caption{AntennaPod.}
    \label{fig:antennapod}
\end{figure}



% FlexboxLayout\footnote{\url{https://github.com/google/flexbox-layout}} is a library project to bring Flexible Box Layout (Flexbox) to Android. Flexbox came from CSS and is a way to structure visual components in a way that their structural positioning in code does not interfere in their presentation. The goal is to predict how the layout is presented in different types of screens.
% The repository for the library also contains sample applications with test suites. 

% \begin{lstlisting}[language=Java, caption=Flexbox Test 1, label=FlexboxTest1]
% @Test
% @FlakyTest
% @Throws(Throwable::class)
% fun testChangeAttributesFromCode() {
%     val activity = activityRule.activity
%     activityRule.runOnUiThread { activity.setContentView(R.layout.recyclerview_reverse) }
%     val recyclerView = activity.findViewById<RecyclerView>(R.id.recyclerview)
%     val layoutManager = recyclerView.layoutManager
%     assertThat(recyclerView, `is`(notNullValue()))
%     assertThat(layoutManager, `is`(instanceOf<Any>(FlexboxLayoutManager::class.java)))
%     val flexboxLayoutManager = layoutManager as FlexboxLayoutManager
%     assertThat(flexboxLayoutManager.flexDirection, `is`(FlexDirection.ROW_REVERSE))
%     assertThat(flexboxLayoutManager.flexWrap, `is`(FlexWrap.WRAP))
%     flexboxLayoutManager.flexDirection = FlexDirection.COLUMN
%     flexboxLayoutManager.flexWrap = FlexWrap.NOWRAP
%     flexboxLayoutManager.justifyContent = JustifyContent.CENTER
%     flexboxLayoutManager.alignItems = AlignItems.FLEX_END
%     assertThat(flexboxLayoutManager.flexDirection, `is`(FlexDirection.COLUMN))
%     assertThat(flexboxLayoutManager.flexWrap, `is`(FlexWrap.NOWRAP))
%     assertThat(flexboxLayoutManager.justifyContent, `is`(JustifyContent.CENTER))
%     assertThat(flexboxLayoutManager.alignItems, `is`(AlignItems.FLEX_END))
% }
% \end{lstlisting}


\subsection{Paintroid}

\sloppy
Paintdroid is a graphical paint editor application for Android implemented in $\sim$25KLOC of Java code.
As of its latest version,\footnote{Revision SHA 1f302a2 as per the time of submission} it had 250 Espresso tests. One of such tests, \texttt{testFullscreenPortraitOrientationChangeWithShape}, checks whether some buttons can be clicked after changing screen orientation. Listing~\ref{PaintroidTest} shows the test. It first selects the Shape drawing option (line~\ref{line:selectShape}), and then sets the screen orientation to portrait (line~\ref{line:setPortrait}). Then, it opens a menu that shows a list of options (line~\ref{line:optionsMenu}), such as saving an image, exporting to a file, saving a copy, etc. The test clicks on the full screen option (line~\ref{line:fullScreen}), then it changes orientation to landscape (line~\ref{setLandscape:fullScreen}), exits full screen mode (line~\ref{line:pressBack}), and clicks on the tool options to again open a menu, and then close it (line~\ref{line:closeToolOptions}). 

\begin{lstlisting}[language=Java, caption=Paintroid Test, label=PaintroidTest,escapechar=|]
@Test
public void testFullscreenPortraitOrientationChangeWithShape() {
 onToolBarView().performSelectTool(ToolType.SHAPE);|\label{line:selectShape}|
 setOrientation(SCREEN_ORIENTATION_PORTRAIT);|\label{line:setPortrait}|
 onTopBarView().performOpenMoreOptions();|\label{line:optionsMenu}|
 onView(withText(R.string.menu_hide_menu)).perform(click());|\label{line:fullScreen}|
 setOrientation(SCREEN_ORIENTATION_LANDSCAPE);|\label{line:setLandscape}|
 pressBack();|\label{line:pressBack}|
 onToolBarView().performOpenToolOptionsView().performCloseToolOptionsView();|\label{line:closeToolOptions}|
}
\end{lstlisting}

The problem associated with this test is that, depending on the processing time needed to show the menu after changing orientation, the click on the menu item might be performed prior to the menu being shown, as Android method calls are mostly asynchronous. \Mar{Leopoldo, for that to happen there needs to be some asynchronicity, no? The test continued execution and left some worker thread opening the menu, right? Can you explain this with greater detail?}. Therefore, it throws the \texttt{PerformException}. Moreover, animations might also impact Espresso tests due to introduced delays.

We ran this test case for 50 times with \rerun\ and found failures in 4 runs (8\% of the cases), showing that this case of flakiness is even more elusive than then one from AntennaPod. Given that it takes 12.5 (=50/4) executions on average to find flakiness and each regular execution takes 5.54s, the aggregate cost to find this flaky test---assuming a developer would run the test for multiple times---is 69.25s (=12.5*5.54). We also ran the test case for 50 times with \tname{} and found failures in 18 runs (26\% of the cases). As the test fails in every 2.77 (=50/18) executions, the aggregate cost of \tname{} to find this flaky test is 19.61s (=2.77x7.08). Overall, \tname{} reveals the flaky test 3.67 times faster than \rerun despite having the execution of the test case itself 1.27 (=7.08/5.54) slower.

%With \tname{}, we were able to find failures in 18 cases, \ie{} 36\%. In this test case, \tname{} increases the chances of finding fault by 4.5 times (8\% versos 36\%). 


\section{\tname{}}
\label{sec:approach}

\sloppy
The goal of \tname{} is to detect flaky tests. The observation that motivates \tname{} is that many flaky tests are flaky because of timing constraints in test executions~\cite{Luo:2014:EAF:2635868.2635920,thorve2018empirical} Our hypotheses are that 1)~such tests can be detected adding noise in the environment where test cases will run and 2)~rerunning tests in a noisy environment is more effective at detecting flakiness compared with rerunning tests without that noise.

\tname{} uses different configurations of a noise generator to detect flaky tests. The process of detecting flaky tests consists of two steps. In the first \emph{offline} step (Section~\ref{technique:discovering-configurations}), \tname{} uses a small sample of tests known to be flaky to search for configurations of a noise generator. In the second \emph{online} step (Section~\ref{technique:finding-flakies}), \tname{} uses those configurations to find time-constrained flaky tests in a project provided as input. The following sections describe these steps in detail.

%The input of \tname{} is a set of tests known to be flaky and the output is a set of configurations for creating noise in the execution environment. \tname{}'s workflow is comprised of two steps. 

%\Mar{@Leopoldo, vc. pode elaborar isto adicionando uma figura e detalhando as subsecoes?}
%\Leo{Ok, vou aproveitar nossa conversa para que eu entenda melhor cada etapa e assim possa explorar melhor o detalhamento do texto. E entao faco uma figura geral, sem problemas.}

\subsection{Noise Generation}
\label{sec:noise-generation}

A noise generator is a tool to create machine load. For example, a noise generator can spawn "stressor" tasks that can influence the execution environment where tests will be executed. Existing tools provide different options for noise generation. We focused on CPU and memory options as we empirically found that these options are important to detect test flakiness (see Section~\ref{sec:answer-rqone}). 

We used the \sng{}~\cite{stress-ng} tool to create noise. \sng{} is an open-source tool designed to exercise various physical subsystems of a computer. We used the following options from \sng{}:

\begin{itemize}[topsep=.2ex,itemsep=.2ex,leftmargin=0.8em]
    \item \textbf{--cpu $n$}. Starts $n$ stressors to exercise the CPU by working sequentially through different CPU stress methods like Ackermann function or Fibonacci sequence.
    \item \textbf{--cpu-load $p$}. Sets the load percentage $p$ for the \textit{--cpu} command.
    \item \textbf{--vm $n$}. Starts $n$ stressors to allocate and deallocate continuously in memory.
    \item \textbf{--vm-bytes $p$}. Sets the size percentage $p$ of total available memory to option\textit{--vm}.
\end{itemize}

For example, the command \CodeIn{stress-ng --cpu 2 --cpu-load 50\% --vm 1 --vm-bytes 30\%} configures \sng{} to run two CPU stressors with 50\% load each and one virtual memory stressor using 30\% of the available memory. The documentation of \sng{} and other examples of use can be found elsewhere\footnote{\url{https://manpages.ubuntu.com/manpages/artful/man1/stress-ng.1.html}}. 

In addition to the options listed above, \tname{} uses an option that we found important for finding flaky Android tests---the number of cores available for use by an Android emulator. This option can be used to restrict an Android emulator to be run on a specified number of cores (smaller than machine's capacity).

In general, a noise generator is \emph{configured} from a list of options $[o_1,...,o_n]$, with each option $o_i$ ranging over the interval $lo_i$-$hi_i$.

%The rationale for selecting this option was that we evaluated \tname{} on Android and this option was found to be relevant for controlling the CPU load.

%\footnote{https://wiki.ubuntu.com/Kernel/Reference/stress-ng}, a specific tool designed to exercise various physical subsystems of a computer, your documentation has several examples and is simple to understand and \Fix{?}\Den{@Marcelo, a outra configuração que usamos não é bem uma ferramenta, apenas setamos a quantidade de cores que o emulador ira utilizar naquela execução, não sei explicar isso bem no texto}. 


\subsection{Step 1: Discovering Configurations}
\label{technique:discovering-configurations}


The input for this phase is a training set of flaky tests $T$ and the output is a set of configurations $c_k=[v_1,..., v_n]$ that maximizes the probability of finding test flakiness in $T$ with \rerun{}. \tname{} uses the tests in $T$ to learn configurations of a noise generator that enables \rerun{} to find flaky tests with higher probability. The flaky tests in $T$ can be obtained by rerunning---for many times---test sets from a sample of applications, known to have flaky tests.

To identify ''good" configurations, it is necessary to define a metric for configuration quality. We use the symbol $\mathit{fit(c_k, T)}$ to denote the fitness value of configuration $c_k$ for the test set $T$. This value measures the average probability of detecting flakiness on a given test suite when we use the configuration $c_k$ for noise generation. For example, a configuration that detects flakiness in $T$ with probabilities $\{0,2, 0.5, 0.0\}$ has $\mathit{fit}(\_,T)$=$(0.2+0.5)/3$=$0.23$. These probabilities are obtained by rerunning the test suite. In this example, one of the tests was found to be flaky in 20\% of the reruns, another test was found to be flaky in 50\% of the reruns, and one of the flaky tests went undetected.

\tname{} divides the search for sets of configurations in two (sub-)steps. First, \tname{} uses a local search to find individual configurations that attempt to maximize the fitness value. Second, \tname{} searches for minimal \emph{sets of configurations} that maximize the probability of detecting flakiness. The rationale is that a configuration can be, individually, very effective at detecting one subset of flaky tests but not very effective at detecting flakiness for another subset of flaky tests.  The following sections elaborate each of these steps in detail.

\subsubsection{Generation of probability matrix}
This step takes on input a set of flaky tests $T$ and reports on output a probability matrix $M$, relating the tests in $T$ and \emph{randomly-sampled} configurations in $K$ by their corresponding probabilities. The symbol $M[t][c]$ denotes the probability of configuration $c\in{}K$ detecting flakiness in $t\in{}T$. To obtain approximate probability measurements, \tname{} runs each test several times on each sampled configuration. The probability measurement $M[t,c]$ is obtained by dividing the number of failures (of $t$ on $c$) found by the total number of reruns (of $t$ on $c$).

\subsubsection{\label{sec:search-configs}Search for sets of configurations} The search for configuration sets can be defined in terms of the Minimum Hitting Set (MHS) problem~\cite{10.1145/1150334.1150336}, a well-known intractable problem with efficient polynomial-time approximations~\cite{DBLP:journals/corr/Gainer-DewarV16}. MHS enables \tname{} to obtain minimum sets of configurations that detect the maximum number of flaky tests. Variations of the MHS problem exist considering weights and returning complete or partial (sub-optimal) solutions~\cite{seedselect}. \tname{} uses the unweighted and complete version of the MHS problem. This version of MHS takes a boolean matrix on input---encoding hi-lo probability of $c$ detecting $t$---and produces minimum hitting sets on output---encoding set of configurations. 

We abstracted the probability matrix $M$ to only encode \emph{likely detected} flaky tests. Intuitively, we are only interested in a configuration $c$ to detect flakiness of a certain test $t$ if the observed probability $M[t][c]$ is above a certain threshold. More precisely, \tname{} computes an abstract matrix $A$ defined as $A[t][c]$=$1$ if $M[t][c]>=\mathit{threshold}$ else $0$. \tname{} runs MHS on the abstract boolean matrix $A$. The goal is to find a set of configurations (columns in the matrix) that detects flakiness in tests (rows in the matrix). 

\vspace{1ex}\noindent\textbf{Example.} Figure~\ref{fig:msh-example} shows an illustrative example of \tname{}'s procedure to discover configurations for detecting flakiness. The left-hand side of the figure shows the original probability matrix as obtained as described above. The abstract matrix appears on the right-hand side. For space, in this case, the test suite contains only three test cases and \tname{} sampled four configurations. In practice, these matrices are much bigger, with hundreds of tests and configurations. The matrix on the left shows the probabilities of these configurations detecting flakiness on $T$ when rerunning each test on each configuration for ten times, totalling 120 (=3x4x10) executions overall. The right side of the figure shows a matrix obtained when using the abstraction function described above with a threshold value of 0.5. There are five hitting sets associated with the abstract matrix, namely $\{c_1, c_2, c_4\}$, $\{c_1, c_3, c_4\}$, $\{c_2, c_3, c_4\}$, $\{c_2, c_4\}$, $\{c_1, c_2, c_3, c_4\}$. The MHS algorithm is able to identify that the set $\{c_2, c_4\}$ is a minimal set that hits (\ie{}, covers) the tests in $T$. In this case, it is also the minimum.

\begin{figure}
    \centering
\begin{tabular}{ccccc}
    %\toprule
          &  $c_1$  & $c_2$   &  $c_3$  & $c_4$ \\
          \cline{2-5}
    $t_1$ &  0.1    &  0.6    &  0.5    &  0.2  \\
    $t_2$ &  0.6    &  0.6    &  0.1    &  0.2  \\
    $t_3$ &  0.1    &  0.1    &  0.1    &  0.5  \\
     \bottomrule
\end{tabular}
~
\hspace{1ex}
\begin{tabular}{ccccc}
    %\toprule
          &  $c_1$  & $c_2$   &  $c_3$  & $c_4$ \\
        \cline{2-5}
    $t_1$ &  0    &  1    &  1    &  0  \\
    $t_2$ &  1    &  1    &  0    &  0  \\
    $t_3$ &  0    &  0    &  0    &  1  \\
     \bottomrule
\end{tabular}
    \caption{Original probability matrix ($M$) and its abstracted version ($A$) using a threshold of 0.5. MHS($A$)=\{$c_2$, $c_4$\}.}
    \label{fig:msh-example}
\end{figure}


\subsection{Step 2: Finding Flakies}
\label{technique:finding-flakies}

Finally, \tname{} uses the configurations obtained on Step 1 to detect test flakiness. It reruns the test suite on each of these configurations for a specified number of times. To facilitate discussion, we use the term \rerunN{} to indicate a variation of \rerun{} that reruns tests in execution environments modified with a specified set of noise configurations.

It is important to note the tension between cost and effectiveness in the comparison of \rerun{} and \rerunN{}. On the one hand, the execution of a test suite with noisy configurations will certainly be more expensive compared to a regular (noiseless) execution as different tasks are competing for the machine resources. On the other hand, we expect that these configurations will be more effective at revealing flaky tests compared to regular configurations. Our hypothesis is that \rerun{} takes longer to find flakies than \rerunN{}, despite its higher cost of one individual rerun.

\begin{table*}[t!]
\small
\setlength{\tabcolsep}{6pt}
\begin{tabular}{rlrrrrlr}
\toprule
\# & App & \#Tests & \#Flakies & \CodeIn{@FlakyTest}(+/-) & \#Stars & GitHub URL (https://github.com/<URL>) & SHA \\ 
\midrule
1 & AntennaPod     & \numTestsAntenna{}   & 12 &  12/0    & 2.8k  & /AntennaPod/AntennaPod                & dd5234c \\
2 & \cellcolor{LightGray}AnyMemo        & 150   &  0   & 0/0      & 115   & /helloworld1/AnyMemo                  & 7e674fb \\
3 & Espresso & 14 & 1 & 1/0 & 1.1k & /TonnyL/Espresso & 043d028 \\
4 & FirefoxLite    & 70   & 15     &  15/3   & 215   & /mozilla-tw/FirefoxLite               & 048d605 \\
5 & Flexbox-layout &  232  & 1   & 0/231        & 15.5k & /google/flexbox-layout                & 611c755 \\
6 & Kiss           & 16    & 3   & 3/0      & 1.5k  & /Neamar/KISS                          & 00011ce \\
7 & \cellcolor{LightGray}Omni-Notes     & 10     & 0     &  0/0    & 2.1k  & /federicoiosue/Omni-Notes             & b7f9396 \\
8 & Orgzly         & 266   & 38      &  38/0  & 1.5k  & /orgzly/orgzly-android                & d74235e \\
9 & Paintroid      & 270   & 5        & 5/0 & 101    & /Catrobat/Paintroid                   & 1f302a2 \\
10 & \cellcolor{LightGray}Susi      & 17   & 0        & 0/0 & 2k    & /fossasia/susi\_android                   & 17a7031 \\
11 & \cellcolor{LightGray}WiFiAnalyzer   & 3     & 0  &  0/0       & 1k    & /VREMSoftwareDevelopment/WiFiAnalyzer & 80e0b5d \\ 
\midrule
Total   &  \multicolumn{1}{c}{-} & 1,298 & 75 (5.78\%) & 74/234 &  \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} \\
\bottomrule
\end{tabular}
\caption{Apps and tests.}
\label{tab:Apps}
\end{table*}

\section{Object of Analyses and Setup}
\label{sec:obj}
\label{sec:setup}

This section describes the methodology we used to build a data set of flaky tests (Section~\ref{sec:objects}) and the setup of \tname{} that we used to run the experiments (Section~\ref{sec:setup}.

\subsection{Objects}
\label{sec:objects}
We mined flaky tests from various GitHub projects to build a dataset to evaluate \tname{}. We used the following search criteria to select projects: 

\begin{enumerate}
    \item the project must be written in Java or Kotlin;
    \item the project must have at least 100 stars;
    \item the project must include tests in Espresso or UIAutomator;
    \item the project needs to be built without errors.
\end{enumerate}

We sampled a total of \numprojects\ projects that satisfied this criteria and used \rerun{} to find test flakiness. More precisely, we executed the test suite of these projects for \numReRuns{} times using a generic Android Emulador (AVD) with Android API version 28. As usual, we consider a test to be flaky if there was a disagreement in the outcomes (\ie, pass, fail, or error) across those runs. For example, we considered as flaky a test that passes in all but one run. To confirm that we could reproduce flakiness, we repeated the execution of each flaky tests for 100 times. (Section~\Fix{xxx} shows results of running \tname{} to find other flaky tests on this same set of projects, \ie{}, flakies that went undetected with 50 non-noisy re-executions of the test suite.) 

Table~\ref{tab:Apps} shows the \numprojects\ projects we used. The column ``App'' shows the name of the Android app, the column ``\#Tests'' shows the number of Espresso or UIAutomator tests in that application, the column ``\#Flakies'' shows the number of flaky tests detected, column ``\CodeIn{@FlakyTest(+/-)} shows numbers x/y with x indicating the number of flaky tests we found that do \emph{not} contain the annotation \CodeIn{@FlakyTest}\footnote{The \CodeIn{@FlakyTest} annotation is a JUnit test filter. It is used on test declarations to indicate JUnit to exclude those tests from execution (if a corresponding command is provided on input).} whereas y indicates the number of tests from the test suite containing the annotation \CodeIn{@FlakyTest} and we missed. Column "\#Stars" shows the number of stars that the project received on GitHub. Finally, columns ``GitHub...'' and ``SHA'' show the GitHub address and corresponding SHA prefix of the revision. 

Results show that we found flaky tests in \numprojectsWithFlakies{} of the \numprojects{} apps, but in two of these apps, namely \CodeIn{Espresso} and \CodeIn{Flexbox-layout}, we found only 1 flaky test on each app. Results also show that the \CodeIn{AntennaPod}, \CodeIn{FirefoxLite}, and \CodeIn{Orgzly} were the apps with the highest number of flaky tests, with 12, 15, and 38 flaky tests, respectively. We found that these projects are among those with the highest number of test cases, with 250, 70, and 266 tests, respectively. Curiously, we found that \CodeIn{Flexbox-layout} is one of the apps with the highest number of tests (232) and lowest number of flakies detected (1). Despite this evidence of determinism on the test suite, developers of this app chose to use the annotation \CodeIn{@FlakyTest} in \emph{all} tests. We inspected the code and it appears that developers used the annotation only for documentation---all tests have that annotation and filtering test cases in that state would result in no test executed.\Mar{@Denini, is there some strong signal that explains why @FlakyTest was used. For example, use of network or sleeps?}. \Den{@Marcelo, eles fazem o testes 'mexendo' nos alementos diretamente na UIThread, acredito que seja isso. O paper do FlakShovel fala um pouco sobre isso: "The problem is that the operation of changing the element
order is performed on the UIThread and position values of the elements are computed asynchronously and updated to View when
the computation is completed." @Leopoldo viu uns testes lá, acho que ele pode complementar; }

Figure~\ref{fig:histogram-testfailures} shows a histogram of number of failures for the \numflakytestsds{} flaky tests we discovered using \rerun. Nearly 52.1\% of the total number of flaky tests revealed failure in 10 or less executions. This number is reflected in the leftmost bar from the histogram. Intuitively, to capture those cases, one would need to rerun the test suite for five times given that the failure probability for that group of tests is 20\% (=10/50). Other studies indicate even smaller probability rates~\cite{bell2018d}.


\begin{figure}[t!]
    \centering
    \includegraphics[trim=0 0 10 50,clip,width=0.4\textwidth]{figs/histogram.png}
    \vspace{-3ex}
    \caption{Histogram of test failures.}
    \label{fig:histogram-testfailures}
\end{figure}

\subsection{Setup}
\label{sec:setup}

\tname{} needs to be trained on a sample of flaky tests (see Section~\ref{technique:discovering-configurations}). We divided our dataset---containing \numflakytestsds\ flaky tests---in a training set (\numflakytraining) and a test set (\numflakytesting). For the training phase, first, we sampled 50 configurations uniformly distributed across the domains of the five parameters we analyzed---four from \sng{} and one from the AVD (See Section~\ref{sec:noise-generation}). Then, we ran each test on each one of these configurations for \numReRunstraining{} times. The result of this execution is a probability matrix with failure probability values 0, 0.33, 0.66, 1.0. We used a probability threshold of \threshold{} to build the abstract matrix as described on Section~\ref{sec:approach}.


%To answer this question we first conduct a training phase, where we choose suitable configurations. Afterwards, we have the testing phase, where we combine configurations. To this end, we split our flaky tests dataset into a training set, consisting of \numTrainingSet{} tests, while the remaining \numTestingSet{} tests are used in the testing phase. 


%Realizamos uma busca manual no Github por "Android" ou "app" com os seguintes criterios: 1) o projeto é feito em Java ou Kotlin. 2) possuem Testes de interface grafica (Espresso ou UIAutomator) e 3) conseguissem "dar build". 





\section{Evaluation}
\label{sec:eval}

The goal of this section is to evaluate the effectiveness of TNAME. We pose the following research questions.

\newcommand{\rqone}{How effective is the introduction of noise in executions to detect flakiness?}
\newcommand{\rqtwo}{How repeatable are the results produced by a given noise configuration?}
\newcommand{\rqthree}{How effective is our search to discover good configurations for noise generation?}
\newcommand{\rqfour}{How effective is \tname\ to find flaky tests?}

\vspace{1ex}
\noindent\textbf{RQ1.} \rqone
\vspace{1ex}

The purpose of this question is to validate the hypothesis that adding noise to test executions influences the probability of detecting a flaky test. To answer this question, we evaluate if using random noisy configurations results in detecting more flaky tests than standard test execution. \Fix{...}

\vspace{1ex}
\noindent\textbf{RQ2.} \rqtwo
\vspace{1ex}

The question analyzes the variance of results obtained with a specific configuration of the noise generator. Optimizing configurations to detect flakies with high probability assume that the variance of results is not very large. In the limit, if results are very non-deterministic then randomly picking a configuration seems the best one could do to find flakies.

\vspace{1ex}
\noindent\textbf{RQ3.} \rqthree
\vspace{1ex}

A good configuration is one that detects lots of flaky tests when enabled. This question evaluates how effective is the search we used to find sets of configurations that lead to increased ability of detecting flaky tests.


\vspace{1ex}
\noindent\textbf{RQ4.} \rqfour
\vspace{1ex}

The rationale for this question is to evaluate how beneficial is \tname{} compared to \rerun. To answer this question, we ran a small controlled experiment where we knew which tests were flaky and measured how long each approach---\tname{} and \rerun---took to discover all flaky tests. We used the same data set to solve the optimization problem and to test it, with this experiment. As usual, we divided the sets of "training" and "testing" flaky tests from the data set to avoid bias. 


The purpose of this question is to evaluate \tname{} on a data set different---and larger---than the one used to discover load configurations. It allows us to measure 1) how quickly \tname{} finds new flaky tests compared to \rerun\ and 2) if it is able to find more flaky tests compared to \rerun.

\subsection{Answering RQ1: \rqone}
\label{sec:answer-rqone}

%\Mar{@Denini, a ideia aqui eh vc. rodar varias configuracoes distintas aleatorias e medir o score. Vc. vai reportar dois conjuntos: C1 e C2. C1 é um conjunto com o score de N execucoes normais e C2 é um conjunto com o score de N execucoes com estas configuracoes distintas. @Leopoldo, vc. usaria este resultado para rodar um experimento estatistico. Faz sentido?} 

\Fix{Explicar o setup.}\Leo{Setup de execução? Neste caso é Denini, correto?}
We ran statistical tests to evaluate if there are differences in the
measurements obtained by randomly introducing noise versus standard 
executions, so we have two paired distributions with 100 executions each. We first ran a 
Shapiro-Wilk test to check if the data is normally distributed. The
p-values of this test are much smaller than $\alpha=0.5$, so we can 
reject the null hypothesis that the data is normally distributed with 
95\% confidence. Given that both data sets are very unlikely normally 
distributed, we used the Wilcoxon signed-rank non-parametric statistical 
hypothesis test to check if there is difference in the measurements. 
The null hypothesis ($H_0$) is that the measurements are the same, that is, 
introducing noise does not impact flakiness detection. The test indicated that
we could reject $H0$ with 95\% confidence as the p-value is less than $0.00001$. 
Given that the distributions were different, we proceeded
to evaluate the effect size. For that we ran a Wilcoxon ranked sum to
measure the effect size, which is given by $r=Z/sqrt(N)$~\cite{r-effect-size}, 
where $N$ is the number of observations in the sample and $Z$ is the Wilcoxon computed
statistic. The magnitude of the effect varies with the value of
$r$---small (0.1), medium (0.3), and large (0.5). Results indicate
that the effect of introducing noise was large ($r$=0.863). Figure~\ref{fig:historgram-random} shows a histogram of the distribution of percentages of flaky tests found with the 100 randomly-selected configurations of the noise generator. Note that, although many configurations detect a relative small number of flakies (0.1-0.2 range), most configurations detect more flakies (avg. 0.32). Noiseless runs detect 5\% (=0.05<0.1) of flakies, on average.
\begin{figure}[t!]
    \centering
    \includegraphics[width=0.4\textwidth]{figs/hist-averages-random.png}
    \vspace{-3ex}
    \caption{Histogram of percentages of flaky tests found with 100 randomly-selected configurations. Vertical line shows average value.}
    \label{fig:historgram-random}
\end{figure}
To sum up: 
\begin{center}
\begin{tcolorbox}[enhanced,width=3in,center upper,drop shadow southwest,sharp corners]
Results indicate that introducing noise positively affects the ability to detect time-constrained flakiness.
\end{tcolorbox}
\end{center}

\subsection{Answering RQ2: \rqtwo}
\label{sec:answer-rqtwo}

The goal of this question is to evaluate how repeatable are the results obtained with a given configuration. Intuitively, if results obtained with two consecutive runs of the test suite with the same configuration are very different then choosing configurations randomly would be no worse than searching for configurations with the goal of maximizing the chances of detecting flakiness (as described in Section~\ref{technique:discovering-configurations}). 

To answer this question, we ran each of the 100 random configurations selected on the previous experiment for 10 times, measuring the percentage of flaky tests found in each execution. For each one of the 100 configurations, we generated a distribution of values corresponding to the percentages of flaky tests detected on a given configuration. 
\begin{figure}[t!]
    \centering
    \includegraphics[width=0.4\textwidth]{figs/sd-random.png}
    \vspace{-3ex}
    \caption{Standard deviations of distributions of 10 reruns on each of the 100 randomly-selected configurations.}
    \label{fig:standard-deviations}
\end{figure}
Figure~\ref{fig:standard-deviations} shows a boxplot summarizing the distribution of standard deviations associated with these 100 distributions. Results indicate that the average and median standard deviation is $\sim$.07, indicating that the average difference in measurements is not superior to 14\% (avg\textpm$\sigma$ for a 95\% confidence interval) of the total number of flaky tests found. We conclude that:
\begin{center}
\begin{tcolorbox}[enhanced,width=3in,center upper,drop shadow southwest,sharp corners]
Despite the non-deterministic nature of the noise generation process, we observed relatively similar percentages of flaky tests detected across re-executions of a given configuration. 
\end{tcolorbox}
\end{center}


\subsection{Answering RQ3: \rqthree}

This research question evaluates whether \tname{} selects good configurations for testing. Section~\ref{technique:discovering-configurations} showed that \tname{} selects configuration in two steps: (1) first, it generates a probability matrix and then (2) it selects configurations from this matrix. The probability matrix relates (a large set of) configurations with the flaky tests from the training set (in our case, \numTrainingSet{} tests) by the probability of a given configuration detecting flakiness on a given test. The focus of this research question is on step 2. More precisely, it evaluates the ability of different techniques to select configurations after the probability matrix has been computed for a given (large) set of randomly-generated configurations. Section~\ref{sec:setup} shows how this set of random configurations is obtained.

%\footnote{The configurations are uniformly sampled from the configuration space. The configuration space consists of the Cartesian product of the domains of variables that control the generation of noise.}. 

We evaluated three strategies for selecting configurations, namely, (i) MHS, (ii) Greedy, and (iii) Random. As described on Section~\ref{sec:search-configs}, MHS is the technique that finds the smallest set of configurations whose execution likely detects all flaky tests from the training set (as per their associated probabilities). Greedy is the technique that selects configurations with maximum individual fitness scores\Mar{check where we define fitness score and refer to the place here}. Finally, Random---our control---is the technique that randomly selects configurations (regardless of their scores). Both Greedy and Random select the same number of configurations as MHS. After generating the configuration sets, we execute the test suite for \numExecutionsRQThree{} times \emph{on the testing set}, measuring the aggregated fitness scores, \ie{} the percentage of flaky tests revealed. To illustrate, suppose MHS produces a set with four configurations and the combined score associated with the execution of the \numflakytesting\ test cases on each of these four configurations (Section~\ref{sec:setup}) revealed 30 flaky tests. In that case, the score of MHS for that particular run will be 75\% (=30/40). The higher the score the better. We use this score to compare the ability of each technique to detect flakiness.

%\Den{@Marcelo, colocar um grafico de barras contendo todas as configurções e seus scores parece ser uma boa opção?}
%\Mar{Acho uma boa ideia, mas a gente precisa de mais execucoes para o histograma fazer sentido. Digamos 50 execucoes...}

We run a statistical test to evaluate if there are differences in the
measurements obtained by MHS, Greedy, and Random. The metric used was the fitness score, as described above, \ie{}, we measured the ratio of flaky tests that each technique detects when executing the test suite on each of the configurations included in the corresponding set. We executed each technique for \numExecutionsRQThree{} times, so each distribution of measurements contain \numExecutionsRQThree{} samples.

As for RQ1, we run a Shapiro-Wilk test to check if the data is normally distributed. We found that the p-values are above $\alpha=0.5$, therefore we concluded that the data is normally distributed. We then used Bartlett's test to check the homogeneity of variances, which reveals that the samples come from  populations with the same variance.
\Mar{Unclear why Bartlett is necessary for (precludes) ANOVA.} 
\Leo{I based myself on the documentation for the one-way ANOVA SciPy function, which states that we need to check for homoscedasticity, which is verified by Bartlett's test. See comment on LaTeX source.}
% The ANOVA test has important assumptions that must be satisfied in order for the associated p-value to be valid.
%     The samples are independent.
%     Each sample is from a normally distributed population.
%     The population standard deviations of the groups are all equal. This property is known as homoscedasticity.
This way, we have used the one-way ANOVA (ANalysis Of VAriance) parametric test to check statistical significance of the sample means by examining the variance of the samples. The null hypothesis ($H_0$) is that there is no variation in the means of sample measurements, which would indicate that there is no impact on changing selection strategies. The p-value we obtained from the ANOVA was significant ($p<0.05$), so we can conclude that there are differences among treatments\Mar{to clarify: confused about the role of ANOVA (as we do a post-hoc analysis later) and whether or not results are contradictory as there are no difference is variances (even for random?)}.\Leo{From what I understand, Bartlett's just checks that data varies in a homogeneous way among the samples, but does not compare the samples directly. ANOVA does this, but since we have three groups we nee to do paired compsrisons} However, we cannot determine which treatments are significantly different from each other only from ANOVA. Therefore, we perform a post-hoc comparison, using the Tukey HSD test to execute multiple pairwise comparisons.\Mar{Suggest to polish table below (in text) and explain results.}\Leo{Still working on the interpretation of the table to put some text here}\Mar{Acho que usamos Tukey aqui. Talvez possa se basear -> https://www.cin.ufpe.br/~damorim/publications/reis-etal-ijcai2019.pdf}\Leo{Não foi Tukey - o teste foi não paramétrico} This test reveals that we have significant differences when comparing Random to both MHS and Greedy, but it does not reject the hypothesis that Greedy and MHS are significantly different.

%                  sum_sq    df          F        PR(>F)
% C(treatments)  0.140487   2.0  66.827696  3.498036e-11
% Residual       0.028380  27.0        NaN           NaN

% Multiple Comparison of Means - Tukey HSD, FWER=0.05 
% ====================================================
% group1 group2 meandiff p-adj   lower   upper  reject
% ----------------------------------------------------
% Greedy    MHS    0.008 0.8352 -0.0279  0.0439  False
% Greedy Random   -0.141  0.001 -0.1769 -0.1051   True
%   MHS Random   -0.149  0.001 -0.1849 -0.1131   True
% ----------------------------------------------------


To sum up: 
\begin{center}
\begin{tcolorbox}[enhanced,width=3.1in,center upper,drop shadow southwest,sharp corners]
Results indicate that there is advantage in selecting configurations based on their fitness scores as opposed to randomly picking them. However, there is no statistical support to claim difference between MHS and Greedy in their abilities to detect flakiness.
\end{tcolorbox}
\end{center}

\subsection{Answering RQ4: \rqfour}

The goal of this research question is to evaluate \tname{}'s performance. To that end, we analyzed two dimensions: (i)~efficiency, \ie{}, how fast it finds flaky tests, and (ii)~completeness, \ie{}, what is the fraction of the set of known flaky tests the technique detects. The set of flaky tests to be detected corresponds to the test set, as defined on Section~\ref{sec:setup}. 

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.4\textwidth]{figs/auc.png}
    \vspace{-3ex}
    \caption{Progress of \tname\ and \rerun\ in detecting flaky tests over time.}
    \label{fig:auc}
\end{figure}

Figure~\ref{fig:auc} shows the progress of \tname\ and \rerun{} in detecting flaky tests over time. The x-axis denotes time in minutes whereas the y-axis denotes the number of flaky tests detected. The steep increase in the number of flaky tests detected by \tname\ indicates that it quickly discovers many flaky tests. For example, 26 of the \numflakytesting\ flaky tests failed (\ie{}, 65\% of the total) in the first execution of the test suite with \tname. (Recall that the test set involves tests of multiple projects.) \rerun{} detected flakiness at a much slower pace, as reflected by the growth of its plot. \rerun\ needed 43 re-executions of the test suite and 316m (=5h16m) to reach saturation with \numFlakyDetectedReRun\ flaky tests detected (\ie{}, \percFlakyDetectedReRun{} of the total) whereas \tname\ needed 14 re-executions and 106m (=1h46m) to reach saturation with \numFlakyDetectedShaker{} flaky tests detected (\ie{}, \percFlakyDetectedShaker{} of the total). The vertical dotted line on Figure~\ref{fig:auc} marks the 32m point in time corresponding to 10\% of the time required by \rerun{} to saturate, which is the limit of the x-axis. In contrast,\Leo{this In contrast is a bit weird, as we are not contrasting with the number of flaky tests found by \rerun{} at that point. Maybe include this information too?} at that point, \tname{} had already found 34 of the flaky tests (\ie{}, 85\% of the total). 

We used the Area Under the Curve (AUC) as a proxy of effectiveness. Rothermel~\etal{}~\cite{Rothermel:ICSM99} pioneered the use of this metric to assess performance of test case prioritization techniques. The larger the area under the curve the better. For these progress plots, a higher area indicates higher ability to detect flaky tests and to detect them quickly. Intuitively, an optimal technique would detect all flaky tests in one execution and would have the AUC of a big trapezoid. Considering the plot from Figure~\ref{fig:auc}, the \rerun{} curve has an AUC of 3,491 where as the \tname{} curve has an AUC of 11,628, \ie{}, the area of \tname\ is 3.33x (=11,628/3,491) higher than that of \rerun. We used the \CodeIn{auc} function of the MESS library in R to compute the AUCs of these plots. That implementation uses the traditional trapezoidal integration method\footnote{https://www.lexjansen.com/nesug/nesug02/ps/ps017.pdf} to obtain AUCs. 

It is worth noting that, because of the increased load in the environment, one execution of the test suite with \tname{} is slower than one execution of the test suite with \rerun. The average cost of one execution of the test suite with \tname\ is 8m55s whereas the average cost of one execution of the test suite with \rerun\ is 7m38s . As \tname\ runs 4 noise generation settings, so the average cost is 35m37s. Despite the increased cost of one execution, overall, \tname{} detects flaky tests considerably faster.

To sum up: 
\begin{center}
\begin{tcolorbox}[enhanced,width=3.1in,center upper,drop shadow southwest,sharp corners]
Results show that (1) \tname\ discovered much more flaky tests than \rerun{} (\percFlakyDetectedShaker{} and \percFlakyDetectedReRun{} of the total, respectively) and (2) discovered these flaky tests much faster. \tname{} discovered 85\% of the total number of possible flaky tests in 10\% of the time \rerun\ took to find its maximum number of flakies.
\end{tcolorbox}
\end{center}


\subsection{Discussion}

\subsubsection{\tname\ in the wild}
\begin{wraptable}[15]{r}{3cm}
\vspace{-4ex}
\centering
\small
\begin{tabular}{lr}
\hline
\toprule
App            & \#Tests \\ 
\midrule
AntennaPod     & 27      \\ 
%AnyMemo        & 0       \\ 
%Espresso       & 0 \\ 
FirefoxLite    & 5 \\ 
Flexbox-layout & 2 \\ 
%Kiss           & 0  \\ 
Omni-Notes     &  4\\ 
Orgzly         & 14 \\ 
Paintroid      & 10 \\ 
%Susi           & 0       \\ 
WiFiAnalyzer   & 1 \\ 
\midrule
Total:         & \numTotalNewFlaky{} \\ 
\bottomrule
\end{tabular}
\caption{New flaky tests found with \tname.}
    \label{tab:new-tests}
\end{wraptable}We ran \tname\ on each project for 5h with the goal of finding additional flaky tests. Table \ref{tab:new-tests} shows the new flaky tests we found. We found new flakies in 7 of the 11 projects (=63.6\%). This experiment revealed a total of \numTotalNewFlaky{} new flaky tests across these projects. We did not find any flaky tests on projects AnyMemo, and Susi, confirming their robustness. \Den{@Marcelo, falta falar o caso para os apps Espresso e KISS, que nao encontramos nada novo, mas tem caso marcado como flaky no experimento anterior}\Leo{Nao entendi, nao seria o caso de mencionar junto com AnyMemo e Susi? Ou estes nao tinham nenhum flaky?}\Den{AnyMemo e Susi não tinham} As per Table~\ref{tab:Apps}, note that the previous experiment we ran could not reveal any flaky tests on these projects either. In contrast, we found one flaky test in Omni-Notes and WifiAnalyzer, projects that previously did not manifest test flakiness. Perhaps not surprising, Orgzly was one of the projects with the highest number of flakies in our previous experiments.


% nome do projeto | numero de testes novos
\Fix{...}

\subsection{Threats to validity}

Threats to the \emph{construct validity} are related to the appropriateness of the evaluation metrics we used. We used popular metrics previously used in related studies. For example, we used the ratio of detected flakiness, the number of flaky tests detected, and the Area Under the Curve (AUC) as metrics to evaluate techniques.  

Threats to the \emph{internal validity} compromise our confidence in establishing a relationship between the independent and dependent variables. To mitigate this threat, we carefully inspected code and scrutinized our results. In addition, we ran our experiments in different machines to confirm the impact of noise in detecting flakiness. Finally, we contacted developers to confirm the flaky tests we detected. 

Threats to the \emph{external validity} relate to the ability to generalize our results. We cannot claim generalization of our results beyond the particular set of projects studied. In particular, our findings are intrinsically limited by projects studied, as well as their domains. Although the studied projects are mostly written in Java\Leo{@Denini: Algum em Kotlin?}\Den{Sim, 3 apps}, we do not expect major differences in results if another language is used.\Leo{Se já tiver algum projeto em Kotlin, talvez não precise nem desta sentença.} The problems we found are related to task coordination. Nevertheless, future work will have to investigate to what extent our findings generalize to software written in other programming languages and frameworks (beyond Android and UI tests).


\section{Related Work}
\label{sec:related}
\label{sec:relatedwork}

\Mar{discuss -->}\Fix{
\cite{ifixflakies}
\cite{idflakies}
\cite{rootCausingflaky}
\cite{userInteractiveTests}
%\cite{refactoringOfTest}
%\cite{empiricallRevisiting}
}

We describe in the following recently related papers to ours.

\subsection{Empirical studies about bugs in test code}
Different empirical studies~\cite{Luo:2014:EAF:2635868.2635920,Vahabzadeh2015,Waterloo:2015,tran2019test} have attempted to characterize  the causes and symptoms of buggy tests, \ie{}, problematic test cases that can fail raising a false alarm when in fact there is no indication of a bug in the application code. This paper focuses on test flakiness, which is one of several possible types of test code issues.
For example, Vahabzadeh \etal{}~\cite{Vahabzadeh2015} mined the JIRA bug repository and the version control systems of Apache Software Foundation and found that 5,556 unique bug fixes exclusively affected test code. They manually examined a sample of 499 buggy tests and found that 21\% of these false alarms were related to flaky tests, which they further classified into Asynchronous Wait, Race Condition, and Concurrency Bugs. In principle all such problems can result in timing constraints that \tname{} could capture. Note, however, that we focused on Android, the diagnosis of defect is out of scope, and that \tname{} is a technique to find these issues whereas the aforementioned works manually analyze test artifacts.
%Recently, Tran et al.~\cite{tran2019test} studied test quality by surveying 19 practitioner's perceptions of test quality and conducting a mining study over the change history of 152 software projects, concluding that testers responsible for test execution are more concerned with comprehension of test cases rather than with their repeatability or performance.

Luo \etal{}~\cite{Luo:2014:EAF:2635868.2635920} analyzed the commit history of the Apache Software Foundation central repository looking for flakiness, specifically. They analyzed 1,129 commits including the keyword ``flak'' or ``intermit'', and then manually inspected each commit. They proposed 10 categories of flakiness root causes and summarized the most common strategies to repair them. Many of the problems reported are related to timing constraints that could, in principle, be captured by \tname{}. We remain to investigate how our \tname\ performs for software of different domains. Thorve \etal~\cite{thorve2018empirical} conducted a study in Android apps and observed that the causes of tests flakiness in Android apps are similar to those identified by Luo~\etal{}~\cite{Luo:2014:EAF:2635868.2635920}. They also found two new causes as Program Logic and UI. 

Altogether, these studies show that test flakiness is prevalent and a potential deterrent to software productivity.

\subsection{Detection of test smells}

Code smells are syntactical symptoms of poor design that could result in a variety of problems. Test smells manifest in test code as opposed to application code. Van Deursen \etal{}~\cite{van2001refactoring} described 11 sources of test smells and suggested corresponding refactorings to circumvent them. More recent studies have been conducted on the same topic. Bavota~\etal{}~\cite{bavota2012empirical} and Tufano~\etal{}~\cite{tufano2016empirical} separately studied the sources of test smells as defined by Van Deursen~\etal{}~\cite{van2001refactoring}. They used simple syntactical patterns to detect these smells in code and then manually inspected them for validation. Bavota~\etal{} found that up to 82\% of the 637 test classes they analyzed contains at least one test smell. In related work, Tufano~\etal{} studied the life cycle of test smells and concluded that they are introduced since test creation---instead of during evolution---and they survive through thousands of commits. Waterloo~\etal{}~\cite{Waterloo:2015} developed a set of (anti-)patterns to pinpoint problematic test code. They performed a study over 12 open source projects to assess the validity of those patterns. The work of Garousi~\etal{}~\cite{garousi2018we} is also worth noting. They prepared a comprehensive catalogue of test smells and a summary of guidelines and tools to deal with them. Test flakiness \emph{may} relate to test smells. For example, the use of sleeps are good predictors of flakiness~\cite{pinto-etal-msr2020,palomba2018automatic}; they induce time constraints that could be violated. We remain to investigate whether the extent to which static methods of flakiness prediction can improve the detection ability of \tname{}. For example, in principle, it is possible to instrument particular tests to initiate and terminate noise generation.

\subsection{Detection of flaky test} 
%We are interested in identifying causes of flakiness as~\cite{Luo:2014:EAF:2635868.2635920,thorve2018empirical}, but we strive for automated and efficient detection of flakiness that could be applied, for example, to warn developers during evolution when they are about to add likely flaky tests. We remain to evaluate how our classifiers perform during evolution. We are particularly interested in understanding developers' reaction to the indication of potential flakiness produced by an IDE in contrast with the alternative approach that indicates flakiness in a report produced by Continuous Integration (CI) systems.

In principle, a test case should produce the same results regardless of the order it is executed in a test suite~\cite{DBLP:conf/issta/ZhangJWMLEN14}. Unfortunately, this is not always the case as the application code that is reached by the test cases can inadvertently modify static area and resetting the static area after the execution of a given test is impractical. Test dependency is one particular source of flakiness~\cite{Luo:2014:EAF:2635868.2635920}. Gambi~\etal{}~\cite{gambi2018practical} proposed a practical approach, based on flow analysis and iterative testing, to detect flakiness due to broken test dependencies. \tname{} is complementary to techniques for capturing broken test dependencies. It remains to investigate how a technique that forcefully modifies the test orderings (\eg{}, discarding tests from test runs and modifying orderings of test execution) compares with the approach proposed by Gambi~\etal.

Bell~\etal{} proposed DeFlaker, a dynamic technique that uses test coverage to detect flakiness during software evolution. DeFlaker observes the latest code changes and marks as flaky any newly failing test that did \emph{not} execute changed code. The expectation is that a test that passed in the previous execution and did not execute changed code should still pass. When that does not happen, DeFlaker assumes that the changes in the coverage profile must have been caused by non-determinism. Note that (1) Deflaker is unable to determine flakiness if the coverage profile was impacted by change and (2) the ability of Deflaker detecting flakiness is bound by the ability of \rerun{} itself.

%Machine learning has also been used to 

Purely static approaches have also been proposed to identify flaky tests~\cite{Herzig:2015,king2018,bertolino2020flaky,pinto-etal-msr2020}. An important benefit of these approaches is scalability. For example, it is possible to build services to proactively search for suspicious tests in open source repositories. On the downside, they only offer estimates of flakiness; re-execution is still necessary to confirm the issue. Herzig and Nagappan~\cite{Herzig:2015} developed a machine learning approach that mines association rules among individual test steps in tens of millions of false test alarms. Lam~\etal{}\cite{king2018} used Bayesian networks for flakiness classification. Pinto~\etal{}~\cite{pinto-etal-msr2020} used binary text classification (\eg{}, Random Forests) to predict test flakiness. They used typical NLP techniques to classify flaky test cases---they tokenized the body of test cases, discarded stop words, put words in camel case, and built language models from the words associated with flaky and non-flaky tests. \tname{} is complementary to static techniques. We remain to evaluate how static classification techniques could be used to selectively run tests in a noisy environment.

%In contrast, our work aims at developing a lightweight flakiness predictor that learns from test code of flaky and non-flaky tests. We are aware of one only recent approach that takes a similar standpoint as we do (i.e.,~\cite{}). However, here, we derive a more comprehensive set of predictors and build a vocabulary of tokens, which is out of their scope.

%The works in~\cite{Herzig:2015,king2018} aim instead to build a \textit{static} predictor, as we also do here. The work in
%\Fix{-------------------------------------------- estou aqui}
%Bell~\etal{}~\cite{bell2018d} and Lam~\etal{}~\cite{lam2019idflakies} proposed different techniques to dynamically detect test flakiness. They require that test cases are executed (one or more times), aiming at optimizing the traditional approach used by practitioners of rerunning failed tests for a fixed number of times. For example, 


\section{Conclusions}
\label{sec:conclusion}

\Fix{...}


\balance
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
\endinput
